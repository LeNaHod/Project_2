{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import FirefoxOptions\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from time import sleep #스크롤다운을 위한 대기시간 sleep\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import requests\n",
    "import json\n",
    "from pyspark.sql.functions import *\n",
    "import requests\n",
    "import json\n",
    "from urllib import parse\n",
    "from urllib.request import urlopen\n",
    "from urllib.request import Request\n",
    "from urllib.error import HTTPError\n",
    "import json\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"idus_final\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1차크롤링(아이디어스에 등록되어있는 상점명만가져오기)\n",
    "base_url = \"https://www.idus.com/c/region/101\"\n",
    "webdriver_service = Service('/home/big/DE_Pr/idus/geckodriver')\n",
    "\n",
    "opts = FirefoxOptions()\n",
    "opts.add_argument(\"--headless\")\t\t\n",
    "browser = webdriver.Firefox(service=webdriver_service, options=opts)\n",
    "\n",
    "#스크롤을내림에 따라 list가증가하니까, 스크롤을 먼저내려줌\n",
    "\n",
    "SCROLL_PAUASE_TIME=0.8\n",
    "\n",
    "browser.get(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    sleep(SCROLL_PAUASE_TIME)\n",
    "    #스크롤을 내려준다\n",
    "    last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    sleep(SCROLL_PAUASE_TIME)\n",
    "    new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        browser.execute_script(\n",
    "            \"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        sleep(SCROLL_PAUASE_TIME)\n",
    "        new_height = browser.execute_script(\t\t\t\n",
    "            \"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        else:\n",
    "            last_height = new_height\n",
    "            continue\n",
    "\n",
    "#DATA-TAGET-ID만 추출하여 리스트안에 넣어놓자\n",
    "classid=browser.find_elements(By.CLASS_NAME,'ui_card--white')\n",
    "\n",
    "classid_copy=[]\n",
    "for comment in classid:\n",
    "    classid_copy.append(comment.get_attribute('data-target-id'))\n",
    "\n",
    "classid_list=classid_copy\n",
    "\n",
    "print('1.scesss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_url=[]\n",
    "no_url=[]\n",
    "num=1\n",
    "for class_name in classid_list:\n",
    "    class_url=f\"https://www.idus.com/c/class/{class_name}\"\n",
    "    browser.get(class_url)\n",
    "    try:\n",
    "        sleep(1.5)\n",
    "        link1=browser.find_element(By.CSS_SELECTOR,'div.artist_card__split a').click()\n",
    "        link2=browser.find_element(By.CSS_SELECTOR,'div.ui_grid__item a').get_attribute('href')\n",
    "        final_url.append(link2)\n",
    "        num=num+1\n",
    "        print(num,'scess')\n",
    "        #print(final_url)\n",
    "    except NoSuchElementException as ns:\n",
    "        no_url_name=browser.find_element(By.CLASS_NAME,'artist-info__name').text\n",
    "        no_url.append(no_url_name)\n",
    "        num=num+1\n",
    "        print(num,'scess')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최초분기지점 (주소 포함, 미포함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#수집(2)\n",
    "\n",
    "final_url_dist = list(dict.fromkeys(final_url))\n",
    "no_url_dist=list(dict.fromkeys(no_url))\n",
    "\n",
    "print(len(final_url)) \n",
    "print(len(final_url_dist)) # 주소 포함된\n",
    "print(len(no_url))\n",
    "print(len(no_url_dist)) # 주소 미포함\n",
    "\n",
    "info_dict={}\n",
    "info_list=[]\n",
    "no_info_list=[]\n",
    "\n",
    "\n",
    "for final_urls in final_url_dist:\n",
    "    browser.get(final_urls)\n",
    "    try:\n",
    "        artist_name=browser.find_element(By.CLASS_NAME,'artist_card__label').text.replace('\\n','')\n",
    "        activetab=browser.find_element(By.XPATH,\"//*[@data-ui-id='info-artist']\").click()\n",
    "        store_name=browser.find_element(By.XPATH,\"//*[@data-panel-id='info-artist']/tbody/tr[1]/td\").text.replace('\\n','')\n",
    "        address=browser.find_element(By.XPATH,\"//*[@data-panel-id='info-artist']/tbody/tr[3]/td\").text.replace('\\n','')\n",
    "        sellernum=browser.find_element(By.XPATH,\"//*[@data-panel-id='info-artist']/tbody/tr[4]/td\").text.replace('\\n','')#번호는 굳이 필요없지만, 가져오지 않을 리스트를 만들기위해 일부러 오류\n",
    "        info_list.append([artist_name,store_name,address])\n",
    "    except NoSuchElementException as e:\n",
    "        artist_name=browser.find_element(By.CLASS_NAME,'artist_card__label').text.replace('\\n','')\n",
    "        no_info_list.append([artist_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주소 포함된 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#수집(3)\n",
    "\n",
    "#로컬에만들어짐(idus_b.csv)\n",
    "\n",
    "# idus홈페이지에서 크롤링한 정보 베이스 csv파일(추후 x,y,경도,위도 추가)\n",
    "# f = open(\"idus_b.csv\", \"w\",encoding='utf-8')\n",
    "# w = csv.writer(f)\n",
    "# w.writerow(['artist_name','store_name','address'])\n",
    "# for cont in info_list:\n",
    "#     w.writerow(cont)  \n",
    "# f.close()\n",
    "# info_list.write.format('csv').mode('append').save('/idus_final/idus_b.csv')\n",
    "\n",
    "# sava csv as hdfs df\n",
    "\n",
    "local_creator_index = [\"artist_name\", \"store_name\", \"address\"]\n",
    "loc_creator = spark.createDataFrame(info_list, local_creator_index)\n",
    "loc_creator.createOrReplaceTempView(\"loc_creator\")\n",
    "loc_creator.write.options(header='True', delimiter=',', encoding=\"utf-8\").csv(\"/local_creator/idus_b.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idus_b = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(\"/local_creator/idus_b.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idus_b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_addr = idus_b.select('address').rdd.map(lambda x:x[0]).collect() # 주소 포함\n",
    "init_artist_name = idus_b.select('artist_name').rdd.map(lambda x:x[0]).collect() # 주소 포함\n",
    "init_store_name = idus_b.select('store_name').rdd.map(lambda x:x[0]).collect() # 주소 포함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_coor =[]\n",
    "for i in init_addr:\n",
    "    Client_ID = \"0dahjnik2r\"\n",
    "    Client_Secret=\"laJOj2DMSCX29PCCXMY6thLXVHtavtomJ1qDQT6t\"\n",
    "    api_url = 'https://naveropenapi.apigw.ntruss.com/map-geocode/v2/geocode?query='\n",
    "    test = i # 주소는 2번째 위치 >> [1]\n",
    "    try:\n",
    "        add_urlenc = parse.quote(test) # URL Encoding\n",
    "    except:\n",
    "        # trick. 변환 불가할 경우 0,0 좌표를 줘서 1km 필터에 걸리지 않도록 처리\n",
    "        latitude, longitude = 0, 0\n",
    "    print(add_urlenc)\n",
    "    url = api_url + add_urlenc\n",
    "    request = Request(url)   \n",
    "    request.add_header('X-NCP-APIGW-API-KEY-ID', Client_ID)\n",
    "    request.add_header('X-NCP-APIGW-API-KEY', Client_Secret)\n",
    "\n",
    "    try:\n",
    "        response = urlopen(request)\n",
    "\n",
    "    except HTTPError as e:\n",
    "        print('HTTP Error')\n",
    "        latitude, longitude = 0, 0\n",
    "\n",
    "    else:\n",
    "        rescode = response.getcode()\n",
    "        \n",
    "        \n",
    "        if rescode == 200:\n",
    "            coordset = []\n",
    "            response_body = response.read().decode('utf-8')\n",
    "            response_body = json.loads(response_body)\n",
    "            \n",
    "            if response_body['addresses'] == []:\n",
    "                coordset.append(0)\n",
    "                coordset.append(0)\n",
    "                market_coor.append(coordset)\n",
    "            else:\n",
    "                latitude = response_body['addresses'][0]['y']\n",
    "                longitude = response_body['addresses'][0]['x']\n",
    "                print('Success')\n",
    "                print(latitude, longitude)\n",
    "                coordset.append(latitude)\n",
    "                coordset.append(longitude)\n",
    "                market_coor.append(coordset)\n",
    "        else:\n",
    "            print(f'Response error, rescode:{rescode}')\n",
    "            latitude, longitude = 0, 0\n",
    "            coordset.append(latitude)\n",
    "            coordset.append(longitude)\n",
    "            market_coor.append(coordset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result check\n",
    "print(init_store_name[0])\n",
    "print((init_artist_name[0]))\n",
    "print((init_addr[0]))\n",
    "print((market_coor[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api 변환 불가지점 확인\n",
    "pop_index_list = []\n",
    "for i in range(len(init_addr)):\n",
    "    if market_coor[i][0] ==0:\n",
    "        pop_index_list.append(i)\n",
    "        print(init_addr[i])\n",
    "# 변환 불가지점 좌표 전환 확인\n",
    "for i in pop_index_list:\n",
    "    print((init_addr)[i])\n",
    "    print((market_coor[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_index = ['artist_name','store_name','address','lot','lat']\n",
    "temp = []\n",
    "for i in range(len(init_artist_name)):\n",
    "    count = Row(init_artist_name[i], init_store_name[i], init_addr[i], float(market_coor[i][0]), float(market_coor[i][1]))\n",
    "    temp.append(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_store_initial_df = spark.createDataFrame(temp, initial_index)\n",
    "c_store_initial_df.write.options(header='True', delimiter=',', encoding=\"utf-8\").csv(\"c_store_initial_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial SAVEPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_test = spark.read.option(\"header\", \"true\").csv(\"c_store_initial_df.csv\")\n",
    "load_test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주소 포함되지 않은 경우\n",
    "* 결측치 f\n",
    "* 미측정값 no_info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 확인과정\n",
    "\n",
    "#print((info_list))\n",
    "#print((no_info_list))\n",
    "for i in info_list:\n",
    "    if i[2] == \"\":\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리(1)-spark\n",
    "\n",
    "# 위에서 생성한 베이스csv파일을 열어서 주소가없는 셀러 추출하여저장\n",
    "\n",
    "\n",
    "# idus_b.printSchema()\n",
    "# idus_b.show(idus_b.count())\n",
    "idus_b.createOrReplaceTempView(\"idus\")\n",
    "# Initial list\n",
    "\n",
    "# 결측치 다룰 df 생성\n",
    "info_no_list = spark.sql(\"\"\"select store_name from idus where address is null\"\"\")\n",
    "info_no_list.write.options(header='True', delimiter=',', encoding=\"utf-8\").csv('/idus_final/info_no_list.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리(2)-python\n",
    "\n",
    "# 다운로드받아야함(info_no_list.csv)\n",
    "# 예외처리에서 나온 목록들을 검색하기위해 1차원리스트로 다 합쳐준다\n",
    "\n",
    "\n",
    "f = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(\"/idus_final/info_no_list.csv\")\n",
    ")  # -> info_no_list.write.format('csv').mode('overwrite').option(\"header\",\"true\").save('/na/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(no_info_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = f.select('store_name').rdd.map(lambda x:x[0]).collect()\n",
    "temp = []\n",
    "for i in no_info_list:\n",
    "    temp.append(i[0])\n",
    "\n",
    "no_address_t = (\n",
    "    data + temp\n",
    ")  # data(idus_b.csv 에서 나온 info_no_list.csv) + no_info_list(수집(2)에서 제외된 리스트 변수)\n",
    "\n",
    "result_no_url = []\n",
    "for no_address_ts in no_address_t:\n",
    "    result_no_url.extend(no_address_ts)\n",
    "\n",
    "result_no_url = no_address_t + no_url_dist  # +수집(1)에서 제외된 리스트 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_no_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수집(4)-kakao rest api(python)\n",
    "\n",
    "# 예외처리 목록들을 카카오api로 검색. 정확도가떨어져 수동확인 완료\n",
    "\n",
    "method = \"GET\"\n",
    "\n",
    "kakaoapi_no_url = result_no_url\n",
    "exception_list = []\n",
    "for kakaoapi_no_urls in kakaoapi_no_url:\n",
    "\n",
    "    params = {\"query\": kakaoapi_no_urls}\n",
    "    url = \"https://dapi.kakao.com/v2/local/search/keyword\"\n",
    "\n",
    "    header = {\"Authorization\": \"KakaoAK 0fda3bab295d6e5f647174eff2af12f7\"}\n",
    "    response = requests.get(url, headers=header, params=params)\n",
    "    tokens = response.json()\n",
    "\n",
    "    try:\n",
    "\n",
    "        tokens_list = list(tokens.values())\n",
    "\n",
    "        tokens_dict = dict(tokens_list[0][0])\n",
    "\n",
    "        artist_name = tokens_dict[\"place_name\"]\n",
    "        address = tokens_dict[\"address_name\"]\n",
    "        lot = float(tokens_dict[\"x\"])\n",
    "        lat = float(tokens_dict[\"y\"])\n",
    "\n",
    "        exception_list.append([artist_name, kakaoapi_no_urls, address, lot, lat])\n",
    "\n",
    "    except:\n",
    "        print(\"PASS : \", kakaoapi_no_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((exception_list[0][4]))\n",
    "print((exception_list[0][3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exception SAVE POINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv\n",
    "\n",
    "c_store_exception_index = ['artist_name','store_name','address','lot','lat']\n",
    "temp2 = []\n",
    "for i in range(len(exception_list)):\n",
    "    count = Row(exception_list[i][0], exception_list[i][1], exception_list[i][2], exception_list[i][3], exception_list[i][4])\n",
    "    temp2.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_store_exception_df = spark.createDataFrame(temp2, c_store_exception_index)\n",
    "c_store_exception_df.createOrReplaceTempView(\"exception_df\")\n",
    "c_store_exception_df.write.options(header='True', delimiter=',', encoding=\"utf-8\").csv(\"c_store_exception_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_test2 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"c_store_exception_df.csv\")\n",
    "load_test2.createOrReplaceTempView(\"c_store_exception_df\")\n",
    "load_test2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## integrate initial, exception df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_store_initial_df.show()\n",
    "c_store_exception_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrated = c_store_initial_df.union(c_store_exception_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrated.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrated_name = integrated.select('store_name').rdd.map(lambda x:x[0]).collect()\n",
    "integrated_lot = integrated.select('lot').rdd.map(lambda x:x[0]).collect()\n",
    "integrated_lat = integrated.select('lat').rdd.map(lambda x:x[0]).collect()\n",
    "store_coor = []\n",
    "for i in range(len(integrated_name)):\n",
    "    temp = []\n",
    "    temp.append(integrated_lot[i])\n",
    "    temp.append(integrated_lat[i])\n",
    "    store_coor.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market = spark.read.option(\"header\", \"true\").option(\"encoding\",\"euc-kr\").csv('market.csv')\n",
    "market.createOrReplaceTempView(\"market\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_x = market.select('x').rdd.map(lambda x:x[0]).collect()\n",
    "market_y = market.select('y').rdd.map(lambda x:x[0]).collect()\n",
    "market_name = market.select('TRDAR_NM').rdd.map(lambda x:x[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_coor=[]\n",
    "for i in range(len(market_x)):\n",
    "    temp=[]\n",
    "    temp.append(float(market_x[i]))\n",
    "    temp.append(float(market_y[i]))\n",
    "    market_coor.append(temp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(store_coor[-1])\n",
    "print(market_coor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install haversine\n",
    "from haversine import haversine\n",
    "\n",
    "final_result = []\n",
    "for j in range(len(market_coor)):\n",
    "    temp = []\n",
    "    temp.append(market_name[j])\n",
    "    store_count = 0\n",
    "    for i in range(len(store_coor)):\n",
    "        point_1 = (store_coor[i][1], store_coor[i][0])\n",
    "        point_2 = (market_coor[j][1], market_coor[j][0])\n",
    "        if haversine(point_1, point_2, unit = 'km') < 1:\n",
    "            store_count += 1\n",
    "    temp.append(store_count)\n",
    "    final_result.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1090\n"
     ]
    }
   ],
   "source": [
    "print(len(final_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_index = ['TRDAR_NM','culture_store_count']\n",
    "temp3 = []\n",
    "for i in range(len(final_result)):\n",
    "    count = Row(final_result[i][0].replace(\",\",\"\"),final_result[i][1])\n",
    "    temp3.append(count)\n",
    "final_df = spark.createDataFrame(temp3, final_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### final result savepoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.write.options(header='True', delimiter=',', encoding=\"utf-8\").csv(\"c_store_final_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_test3 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"c_store_final_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-08 23:25:40,868 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2022-12-08 23:25:40,868 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2022-12-08 23:25:40,868 INFO datasources.FileSourceStrategy: Output Data Schema: struct<TRDAR_NM: string>\n",
      "2022-12-08 23:25:40,869 INFO memory.MemoryStore: Block broadcast_551 stored as values in memory (estimated size 303.0 KiB, free 433.7 MiB)\n",
      "2022-12-08 23:25:40,877 INFO storage.BlockManagerInfo: Removed broadcast_550_piece0 on ubuntu:42485 in memory (size: 6.8 KiB, free: 434.3 MiB)\n",
      "2022-12-08 23:25:40,881 INFO memory.MemoryStore: Block broadcast_551_piece0 stored as bytes in memory (estimated size 53.8 KiB, free 433.7 MiB)\n",
      "2022-12-08 23:25:40,881 INFO storage.BlockManagerInfo: Added broadcast_551_piece0 in memory on 192.168.254.129:43935 (size: 53.8 KiB, free: 434.3 MiB)\n",
      "2022-12-08 23:25:40,881 INFO spark.SparkContext: Created broadcast 551 from javaToPython at <unknown>:0\n",
      "2022-12-08 23:25:40,881 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194675 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2022-12-08 23:25:40,885 INFO storage.BlockManagerInfo: Removed broadcast_550_piece0 on 192.168.254.129:43935 in memory (size: 6.8 KiB, free: 434.3 MiB)\n",
      "2022-12-08 23:25:40,900 INFO storage.BlockManagerInfo: Removed broadcast_549_piece0 on 192.168.254.129:43935 in memory (size: 6.8 KiB, free: 434.3 MiB)\n",
      "2022-12-08 23:25:40,901 INFO storage.BlockManagerInfo: Removed broadcast_549_piece0 on ubuntu:43903 in memory (size: 6.8 KiB, free: 434.3 MiB)\n",
      "2022-12-08 23:25:40,904 INFO storage.BlockManagerInfo: Removed broadcast_548_piece0 on ubuntu:43903 in memory (size: 53.8 KiB, free: 434.4 MiB)\n",
      "2022-12-08 23:25:40,905 INFO storage.BlockManagerInfo: Removed broadcast_548_piece0 on ubuntu:42485 in memory (size: 53.8 KiB, free: 434.4 MiB)\n",
      "2022-12-08 23:25:40,908 INFO storage.BlockManagerInfo: Removed broadcast_548_piece0 on 192.168.254.129:43935 in memory (size: 53.8 KiB, free: 434.3 MiB)\n",
      "2022-12-08 23:25:40,913 INFO spark.SparkContext: Starting job: collect at /tmp/ipykernel_4149/3284381193.py:2\n",
      "2022-12-08 23:25:40,913 INFO scheduler.DAGScheduler: Got job 319 (collect at /tmp/ipykernel_4149/3284381193.py:2) with 2 output partitions\n",
      "2022-12-08 23:25:40,913 INFO scheduler.DAGScheduler: Final stage: ResultStage 319 (collect at /tmp/ipykernel_4149/3284381193.py:2)\n",
      "2022-12-08 23:25:40,913 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2022-12-08 23:25:40,913 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2022-12-08 23:25:40,913 INFO scheduler.DAGScheduler: Submitting ResultStage 319 (PythonRDD[1335] at collect at /tmp/ipykernel_4149/3284381193.py:2), which has no missing parents\n",
      "2022-12-08 23:25:40,914 INFO memory.MemoryStore: Block broadcast_552 stored as values in memory (estimated size 16.7 KiB, free 434.0 MiB)\n",
      "2022-12-08 23:25:40,915 INFO memory.MemoryStore: Block broadcast_552_piece0 stored as bytes in memory (estimated size 8.8 KiB, free 434.0 MiB)\n",
      "2022-12-08 23:25:40,916 INFO storage.BlockManagerInfo: Added broadcast_552_piece0 in memory on 192.168.254.129:43935 (size: 8.8 KiB, free: 434.3 MiB)\n",
      "2022-12-08 23:25:40,916 INFO spark.SparkContext: Created broadcast 552 from broadcast at DAGScheduler.scala:1478\n",
      "2022-12-08 23:25:40,916 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 319 (PythonRDD[1335] at collect at /tmp/ipykernel_4149/3284381193.py:2) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2022-12-08 23:25:40,916 INFO cluster.YarnScheduler: Adding task set 319.0 with 2 tasks resource profile 0\n",
      "2022-12-08 23:25:40,916 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 319.0 (TID 466) (ubuntu, executor 2, partition 0, NODE_LOCAL, 4944 bytes) taskResourceAssignments Map()\n",
      "2022-12-08 23:25:40,916 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 319.0 (TID 467) (ubuntu, executor 1, partition 1, NODE_LOCAL, 4944 bytes) taskResourceAssignments Map()\n",
      "2022-12-08 23:25:40,920 INFO storage.BlockManagerInfo: Added broadcast_552_piece0 in memory on ubuntu:43903 (size: 8.8 KiB, free: 434.4 MiB)\n",
      "2022-12-08 23:25:40,925 INFO storage.BlockManagerInfo: Added broadcast_552_piece0 in memory on ubuntu:42485 (size: 8.8 KiB, free: 434.4 MiB)\n",
      "2022-12-08 23:25:40,926 INFO storage.BlockManagerInfo: Added broadcast_551_piece0 in memory on ubuntu:43903 (size: 53.8 KiB, free: 434.3 MiB)\n",
      "2022-12-08 23:25:40,938 INFO storage.BlockManagerInfo: Added broadcast_551_piece0 in memory on ubuntu:42485 (size: 53.8 KiB, free: 434.3 MiB)\n",
      "2022-12-08 23:25:40,979 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 319.0 (TID 466) in 63 ms on ubuntu (executor 2) (1/2)\n",
      "2022-12-08 23:25:40,980 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 319.0 (TID 467) in 64 ms on ubuntu (executor 1) (2/2)\n",
      "2022-12-08 23:25:40,980 INFO cluster.YarnScheduler: Removed TaskSet 319.0, whose tasks have all completed, from pool \n",
      "2022-12-08 23:25:40,981 INFO scheduler.DAGScheduler: ResultStage 319 (collect at /tmp/ipykernel_4149/3284381193.py:2) finished in 0.067 s\n",
      "2022-12-08 23:25:40,981 INFO scheduler.DAGScheduler: Job 319 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2022-12-08 23:25:40,981 INFO cluster.YarnScheduler: Killing all running tasks in stage 319: Stage finished\n",
      "2022-12-08 23:25:40,981 INFO scheduler.DAGScheduler: Job 319 finished: collect at /tmp/ipykernel_4149/3284381193.py:2, took 0.067813 s\n"
     ]
    }
   ],
   "source": [
    "#load_test3.show()\n",
    "asdf = load_test3.select('TRDAR_NM').rdd.map(lambda x:x[0]).collect()\n",
    "print(len(asdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
